# Automatic Lens Correction â€” A100-Optimized CNN

A high-performance deep learning pipeline for automatic lens distortion correction using EfficientNetB0 and differentiable Brown-Conrady undistortion, optimized for NVIDIA A100 GPUs.

## ğŸ¯ Overview

This project trains a CNN to predict lens distortion coefficients (kâ‚, kâ‚‚, pâ‚, pâ‚‚) from distorted images and applies differentiable undistortion to recover ground truth geometry. The implementation is specifically optimized for Colab Pro A100 instances with a **10-minute end-to-end pipeline**.

### Key Features

- **bfloat16 Mixed Precision**: 3Ã— throughput vs float32 on A100, no loss scaling needed
- **XLA JIT Compilation**: 20â€“40% additional speedup via kernel fusion
- **EfficientNetB0 Backbone**: Pretrained ImageNet weights, efficient feature extraction
- **Differentiable Geometry**: Brown-Conrady distortion model with backprop-safe sampling
- **Two-Phase Training**: Frozen backbone â†’ progressive fine-tuning
- **Memory-Safe Pipeline**: ~1.3 GB peak RAM via tf.data + local SSD streaming
- **Parallel I/O**: gsutil bulk download + native TF decode (15â€“20Ã— faster than GCS Python clients)

## ğŸ“Š Architecture
Distorted Image (384Ã—384) â†“ 
[CNN Encoder] â€¢ EfficientNetB0 (pretrained) â€¢ Global Average Pooling â†’ (1280,) â†“ 
[Regression Head] â€¢ Dense(512) + Dropout(0.3) â€¢ Dense(128) + Dropout(0.2) â€¢ Dense(4, tanh) + Scale â†“ 
[kâ‚, kâ‚‚, pâ‚, pâ‚‚] coefficients â†“ 
[Differentiable Brown-Conrady Undistortion] â†“
Undistorted Image (384Ã—384) â†“
Loss = 0.8 Ã— (1 - SSIM) + 0.2 Ã— L1


## âš™ï¸ Configuration

| Parameter | Value | Notes |
|-----------|-------|-------|
| **Input Size** | 224Ã—224 | CNN input (EfficientNetB0 standard) |
| **Undistort Size** | 384Ã—384 | Higher res = better geometric detail |
| **Batch Size** | 64 | Fills A100 40GB VRAM |
| **Epochs** | 15 | ~15s/epoch on A100 |
| **Learning Rate** | 3e-4 | Linear scaling for batch size 64 |
| **Phase 2 Start** | Epoch 6 | Unfreeze top-60 backbone layers |
| **Loss Alpha** | 0.8 | 80% SSIM + 20% L1 |
| **Early Stop Patience** | 3 | Stop if val SSIM doesn't improve |

## ğŸ“ˆ Expected Performance (A100 Colab)

| Stage | Time |
|-------|------|
| GCS download | 2â€“3 min |
| Training (15 epochs) | 3â€“5 min |
| Evaluation + ZIP | 1â€“2 min |
| **Total** | **6â€“10 min** âœ“ |

### Per-Epoch Breakdown
- **Epoch 1**: ~25s (XLA JIT compilation)
- **Epochs 2â€“5**: ~8s each (Phase 1: head only)
- **Epochs 6â€“15**: ~15â€“20s each (Phase 2: fine-tune)

## ğŸš€ Quick Start

### Prerequisites
- Google Colab Pro with A100 GPU access
- GCP project with GCS bucket containing lens distortion datasets
- `tensorflow >= 2.14`, `opencv-python`, `scikit-image`

### Installation

```python
# In Colab cell 1:
from google.colab import auth
auth.authenticate_user()

# Run the full notebook (it handles all setup)
Usage
Update Config (cell with CONFIG section):

Python
GCP_PROJECT_ID      = "your-project-id"
GCS_BUCKET_NAME     = "your-bucket"
GCS_TRAIN_FULL_PATH = "path/to/training/images/"
GCS_TEST_FULL_PATH  = "path/to/test/images/"
Run the notebook top to bottom:

Step 1: Downloads training/test images via gsutil -m cp
Step 2: Loads image helpers
Step 3: Builds differentiable undistortion layer
Step 4: Constructs CNN model
Step 5: Defines loss functions
Step 6: Creates tf.data pipeline
Step 7: Two-phase training loop
Step 8: Evaluation + visualization
Outputs saved to ./output/:

lens_cnn_model_a100.keras â€” trained model
lens_correction_cnn_a100.zip â€” predictions + side-by-side comparisons
training_curves_a100.png â€” loss/SSIM plots
ğŸ”§ Advanced Tuning
For T4 GPUs (Colab Free)
Python
# Reduce resolution (less memory needed)
UNDISTORT_SIZE = 256
BATCH_SIZE = 32

# Use float16 with loss scaling
tf.keras.mixed_precision.set_global_policy('mixed_float16')
optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,
                                      loss_scale='dynamic')
For RTX 3090 / 4090 (Local)
Python
BATCH_SIZE = 128
EPOCHS = 20
LEARNING_RATE = 1e-3
UNDISTORT_SIZE = 512
ğŸ“š Key Implementation Details
Why gsutil -m cp?
Direct I/O optimization: Google's own CLI tool uses C extensions + XML multipart API
Automatic parallelization: Splits work across multiple OS threads
No Python bottleneck: GIL-free, region-aware routing
Result: ~23,000 image pairs downloaded in 60â€“90 seconds
Why bfloat16?
Wide exponent range: Same as float32, no underflow â†’ no LossScaleOptimizer needed
Native A100 support: 3Ã— faster than float32 on tensor cores
Stable training: No loss scaling artifacts with plain Adam
Why XLA?
Kernel fusion: Combines individual GPU ops into larger, optimized kernels
20â€“40% speedup: On top of bfloat16 benefits
First-call overhead: ~10s compilation, cached thereafter
Why two-phase training?
Phase 1 (head only): Quick convergence on new task, ~8s/epoch
Phase 2 (fine-tune): Adapt ImageNet features to lens distortion, ~15s/epoch
BatchNorm frozen: Preserves ImageNet statistics in backbone
ğŸ“Š Data Format
Training Data
Code
gs://bucket/path/
â”œâ”€â”€ image_001_original.jpg    (distorted)
â”œâ”€â”€ image_001_generated.jpg   (ground truth, undistorted)
â”œâ”€â”€ image_002_original.jpg
â”œâ”€â”€ image_002_generated.jpg
â””â”€â”€ ...
Test Data
Code
gs://bucket/path/
â”œâ”€â”€ test_001.jpg
â”œâ”€â”€ test_002.jpg
â””â”€â”€ ...
ğŸ“ Loss Function
Code
Loss = Î± Ã— (1 - SSIM) + (1 - Î±) Ã— L1

where:
  SSIM  : Structural Similarity Index (rewards geometric accuracy)
  L1    : Mean Absolute Pixel Error (prevents zero-coefficient degenerate solutions)
  Î±     : 0.8 (80% SSIM, 20% L1)
ğŸ“ Output Coefficients
The model predicts Brown-Conrady distortion parameters scaled to physical ranges:

Coefficient	Range	Meaning
kâ‚	[-1.0, 1.0]	Primary radial distortion
kâ‚‚	[-0.5, 0.5]	Secondary radial distortion
pâ‚	[-0.1, 0.1]	Tangential distortion (x-axis)
pâ‚‚	[-0.1, 0.1]	Tangential distortion (y-axis)
ğŸ” Evaluation Metrics
SSIM (Structural Similarity): Primary metric â€” higher is better [0, 1]
L1 Loss: Mean absolute pixel error [0, 255]
Validation SSIM: Reported per epoch; model saved when improved
ğŸ“¦ Dependencies
Code
tensorflow >= 2.14
numpy
pandas
opencv-python
scikit-image
matplotlib
google-cloud-storage
google-auth
Pillow
tqdm
psutil
ğŸ› Troubleshooting
"âŒ No GPU detected!"
â†’ Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ A100 GPU â†’ Save

gsutil auth errors
â†’ Re-run auth.authenticate_user() at the top, then cell with gsutil commands

OOM (Out of Memory)
Python
# Reduce batch size or resolution
BATCH_SIZE = 32
UNDISTORT_SIZE = 256
Slow epoch times
â†’ Ensure A100 selected (not T4); check GPU with nvidia-smi

ğŸ“„ References
Brown-Conrady Distortion Model
EfficientNet
bfloat16 on A100
XLA: Optimizing Compiler for TensorFlow
ğŸ“„ License
[Add your license here]

ğŸ‘¤ Author
Deepthi V J
Joshua Jose
